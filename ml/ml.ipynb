{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c77c1f9a-2a9e-4c46-ae7f-002f215ff2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "sys.path.append('../')\n",
    "import fn\n",
    "from addition_dataset import GroupAddition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e1c632-414c-46b3-a7c1-bcdf9d06f54b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb94066-33ea-4bbb-8d4c-33aebccfa958",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da3c20fc-e92f-4a2f-a3d2-ce31a9e0b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(b, depth, split_type, split_ratio=0.9, split_depth=-1):\n",
    "    assert split_type in ['interpolate', 'OOD'], 'invalid type'\n",
    "    N = b**depth\n",
    "    if split_type == 'interpolate':\n",
    "        assert (0 < split_ratio < 1), 'invalid split'\n",
    "        ids = random.sample(range(N), math.ceil(split_ratio * N))\n",
    "    elif split_type == 'OOD':\n",
    "        assert (0 < split_depth <= depth), 'invalid sample_depth'\n",
    "        ids = list(range(b**split_depth))\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b537415-1d10-4cf3-9071-22fecf420b73",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(b, depth, table, batch_size=16, num_passes=1000, split_type='interpolate', split_ratio=0.9, split_depth=-1):\n",
    "    \n",
    "    # Get indices of training and testing data\n",
    "    N = b**depth\n",
    "    ids = get_ids(b, depth, split_type, split_ratio, split_depth)\n",
    "    heldout_ids = set(range(N)) - set(ids)\n",
    "    \n",
    "    # Create training dataset and dataloader\n",
    "    training_dataset = GroupAddition(table, depth, ids=ids, interleaved=True, digit_order='reversed')\n",
    "    training_dataset = torch.utils.data.ConcatDataset([training_dataset] * num_passes)\n",
    "    training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Create testing dataset and dataloader\n",
    "    testing_dataset = GroupAddition(table, depth, ids=heldout_ids, interleaved=True, digit_order='reversed')\n",
    "    testing_dataloader = torch.utils.data.DataLoader(testing_dataset)\n",
    "\n",
    "    return training_dataloader, testing_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ad8d3-1ba6-4427-bfd5-f953a2af70f3",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55fd157-1bd9-48b5-8777-947346ed2d11",
   "metadata": {},
   "source": [
    "### Models and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2044b98f-1d03-4eac-af03-b390ba005c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class LSTMModel(nn.Module):\n",
    "    '''Simple LSTM model for testing purposes'''\n",
    "    def __init__(self, b, layers):\n",
    "        super().__init__()\n",
    "        self.b = b\n",
    "        self.layers = layers\n",
    "        self.lstm = nn.LSTM(b, b, layers, batch_first=True)\n",
    "        self.linear = nn.Linear(b, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X_out, _ = self.lstm(X)\n",
    "        X_out = self.linear(X_out).squeeze()\n",
    "        return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b39d82-0657-40a4-815c-cc97b38ee457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    '''Positional encoder for transformer'''\n",
    "    def __init__(self, b, max_len: int = 300):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, b, 2) * (-math.log(10000.0) / b))\n",
    "        pe = torch.zeros(max_len, 1, b)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X_out = X + self.pe[:X.size(0)]\n",
    "        return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1891403b-54ab-4dad-a566-168baf0b8a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class TransformerModel(nn.Module):\n",
    "    '''Simple transformer model for testing purposes'''\n",
    "    def __init__(self, b, heads, layers):\n",
    "        super().__init__()\n",
    "        self.b = b\n",
    "        self.heads = heads\n",
    "        self.layers = layers\n",
    "        self.pos_encoder = PositionalEncoding(b)\n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(b, heads, batch_first=True), layers)\n",
    "        self.linear = nn.Linear(b, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X_out = self.pos_encoder(X)\n",
    "        X_out = self.transformer(X_out)\n",
    "        X_out = self.linear(X_out).squeeze()\n",
    "        return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af33aa9-7a42-4e34-a433-d1383fd619b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X_out, ids):\n",
    "    if X_out.dim() == 2:\n",
    "        X_out_and_ids = zip(torch.unbind(X_out), torch.unbind(ids))\n",
    "        s_out = torch.stack([X_out[ids] for X_out, ids in X_out_and_ids])\n",
    "    else:\n",
    "        s_out = X_out[ids]\n",
    "    return s_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63add8ce-fab3-4d53-af88-ed8a60a7c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X_out, s, ids):\n",
    "        MSE = nn.MSELoss()\n",
    "        s_out = prediction(X_out, ids)\n",
    "        loss = MSE(s_out, s)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce717502-3ef2-4d2c-adf0-aded96ab6736",
   "metadata": {},
   "source": [
    "### Training and testing loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c0867f-0a24-496d-8beb-1b2e7f70f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_dataloader, criterion, optimizer, print_losses=True):\n",
    "\n",
    "    # Initialize variables\n",
    "    losses = []\n",
    "    training_acc = []\n",
    "    testing_acc = []\n",
    "    t = 0\n",
    "\n",
    "    # Training loop\n",
    "    for batch_idx, (X, s, ids) in enumerate(training_dataloader):\n",
    "        \n",
    "        # Compute, store, and print loss\n",
    "        loss = criterion(model(X), s.float(), ids)\n",
    "        losses.append(loss.item())                \n",
    "        if print_losses and (t % 100 == 0):\n",
    "            print(f't = {t}  loss = {loss.item():.6f}')\n",
    "\n",
    "        # Compute and store training and testing accuracies\n",
    "        # if t % 10 == 0:\n",
    "        #     with torch.no_grad():\n",
    "        #         model.eval()\n",
    "        #         testing_acc = \n",
    "        \n",
    "        # Zero gradients, perform a backward pass, and update the weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Iterate counter\n",
    "        t += 1\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d21421f6-add5-4340-b7bb-99ff5853e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "def test(model, testing_dataset, print_accuracy=True):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "    \n",
    "        # Perform evaluation\n",
    "        total_correct = 0\n",
    "        for i, (X, s, ids) in enumerate(testing_dataset):\n",
    "            \n",
    "            # Forward pass\n",
    "            X_out = model(X)\n",
    "            s_out = prediction(X_out, ids)\n",
    "            s_out = torch.round(s_out)\n",
    "    \n",
    "            # Calculate accuracy\n",
    "            total_correct += ((s_out == s).sum() == s.shape[1]).item()\n",
    "\n",
    "        total_samples = len(testing_dataset)\n",
    "        accuracy = total_correct / total_samples\n",
    "        if print_accuracy:\n",
    "            print(f'Accuracy on testing set: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "450f3df9-078b-4dc8-b976-5468433aea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/all_tables.pickle', 'rb') as f:\n",
    "    all_tables = pickle.load(f)\n",
    "tables = all_tables[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "285c8155-b409-4d61-9ab8-2bf740672156",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tables[((0, 0, 3, 2), (0, 1, 1, 1), (0, 2, 3, 0), (0, 3, 1, 3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b8c310a7-ec27-4482-8d82-355dfaae0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader, testing_dataloader = prepare_data(4, 7, table, 16, num_passes=1000, split_type='OOD', split_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c6eb1918-bbbe-4999-890e-312078623dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07feec9b-fd4f-4411-93d7-c737c4bc2a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "193c7fae-2264-43c0-a797-f4bd8f2782df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0  loss = 1.653278\n",
      "t = 100  loss = 0.998897\n",
      "t = 200  loss = 0.791416\n",
      "t = 300  loss = 0.612122\n",
      "t = 400  loss = 0.655371\n",
      "t = 500  loss = 0.675804\n",
      "t = 600  loss = 0.452836\n",
      "t = 700  loss = 0.511770\n",
      "t = 800  loss = 0.472459\n",
      "t = 900  loss = 0.281568\n",
      "t = 1000  loss = 0.266343\n",
      "t = 1100  loss = 0.111235\n",
      "t = 1200  loss = 0.113442\n",
      "t = 1300  loss = 0.155005\n",
      "t = 1400  loss = 0.066100\n",
      "t = 1500  loss = 0.054853\n",
      "t = 1600  loss = 0.113622\n",
      "t = 1700  loss = 0.085822\n",
      "t = 1800  loss = 0.050935\n",
      "t = 1900  loss = 0.052865\n",
      "t = 2000  loss = 0.099884\n",
      "t = 2100  loss = 0.026707\n",
      "t = 2200  loss = 0.042450\n",
      "t = 2300  loss = 0.042581\n",
      "t = 2400  loss = 0.025130\n",
      "t = 2500  loss = 0.018015\n",
      "t = 2600  loss = 0.027794\n",
      "t = 2700  loss = 0.035593\n",
      "t = 2800  loss = 0.005351\n",
      "t = 2900  loss = 0.010704\n",
      "t = 3000  loss = 0.010675\n",
      "t = 3100  loss = 0.008102\n",
      "t = 3200  loss = 0.006214\n",
      "t = 3300  loss = 0.015836\n",
      "t = 3400  loss = 0.010916\n",
      "t = 3500  loss = 0.007532\n",
      "t = 3600  loss = 0.006070\n",
      "t = 3700  loss = 0.010918\n",
      "t = 3800  loss = 0.006415\n",
      "t = 3900  loss = 0.004926\n"
     ]
    }
   ],
   "source": [
    "losses = train(model, training_dataloader, criterion, optimizer, print_losses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "05551e53-99e0-41d3-918c-2428e540f4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing set: 0.9942\n"
     ]
    }
   ],
   "source": [
    "test(model, testing_dataloader, print_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee69f21a-f86e-406d-a467-260a28913068",
   "metadata": {},
   "source": [
    "Exciting!! Evidence that LSTM can learn the abstract carry rule and extend to OOD samples. Above, trained on 3-digit addition and when tested up to 7-digits attains ~100% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7146a-dd51-4fa2-bb77-f9bb06d8e382",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "864b3cef-e275-4b1c-9e36-89c26fdc3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(losses, size=10):\n",
    "    kernel = np.ones(size) / size\n",
    "    smoothed_losses = np.convolve(losses, kernel, mode='same')\n",
    "    return smoothed_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cd5c9da-585d-41c4-b56c-70cffb8e266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(all_losses, size=10):\n",
    "    if len(all_losses) > 1:\n",
    "        alpha = 0.5\n",
    "    for c, losses in all_losses.items():\n",
    "        plt.plot(smooth(losses, size=size), alpha=alpha, label=sorted(c)[0])\n",
    "    if len(all_losses) > 1:\n",
    "        plt.legend()\n",
    "    plt.title(f'Epoch vs. Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b4973b70-a7f8-42da-9467-bf26d9170184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_table_losses(b, depth, batch_size, architecture):\n",
    "\n",
    "    # Assert architecture is valid\n",
    "    assert architecture in ['LSTM', 'transformer']\n",
    "    \n",
    "    # Get carry tables for base\n",
    "    with open('../pickles/all_tables.pickle', 'rb') as f:\n",
    "        all_tables = pickle.load(f)\n",
    "    tables = all_tables[b]\n",
    "    \n",
    "    # Train model for each table\n",
    "    all_losses = {}\n",
    "    for c, table in tables.items():\n",
    "    \n",
    "        # Get dataloaders\n",
    "        training_dataloader, testing_dataloader = prepare_data(b, depth, table, batch_size=batch_size)\n",
    "        \n",
    "        # Initialize model\n",
    "        if architecture == 'LSTM':\n",
    "            model = LSTMModel(b, 2)\n",
    "        elif architecture == 'transformer':\n",
    "            model = TransformerModel(b, 1)\n",
    "        \n",
    "        # Define criterion and optimizer\n",
    "        criterion = Loss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "        # Train and test model\n",
    "        losses = train(model, training_dataloader, criterion, optimizer, print_losses=False)\n",
    "        print(c)\n",
    "        test(model, testing_dataloader, depth, print_accuracy=True)\n",
    "        print()\n",
    "        all_losses[c] = losses\n",
    "\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c5e41-96cf-4495-8a97-45c17fbc6d57",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1e1d4-2ec4-4cff-a043-0138c7a6f8b3",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5767687-2051-435a-b159-5a693f85b695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 0, 3, 2), (0, 1, 1, 1), (0, 2, 3, 0), (0, 3, 1, 3))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_losses \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_table_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLSTM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 31\u001b[0m, in \u001b[0;36mcompute_table_losses\u001b[0;34m(b, depth, batch_size, architecture)\u001b[0m\n\u001b[1;32m     29\u001b[0m losses \u001b[38;5;241m=\u001b[39m train(model, training_dataloader, criterion, optimizer, print_losses\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(c)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_accuracy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     33\u001b[0m all_losses[c] \u001b[38;5;241m=\u001b[39m losses\n",
      "Cell \u001b[0;32mIn[29], line 19\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, testing_dataset, depth, print_accuracy)\u001b[0m\n\u001b[1;32m     16\u001b[0m     s_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(s_out)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43m(\u001b[49m\u001b[43ms_out\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m depth)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     21\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(testing_dataset)\n\u001b[1;32m     22\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m total_correct \u001b[38;5;241m/\u001b[39m total_samples\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "all_losses = compute_table_losses(4, 3, 16, 'LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d55d6e-df5b-48cb-94ca-64fa2bc10315",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(all_losses, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc624dff-28b8-4ffe-9ab3-2f991d399f08",
   "metadata": {},
   "source": [
    "The LSTM does well with interleaved, reversed data (still need to check for harder formats). Able to achieve close to perfect accuracy on unseen tuples for all carry tables, seems like standard table loss converges more quickly than alternatives but want more robust data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b022fc15-8688-455d-a7af-742b50ea16cc",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c574dc5a-7617-419a-bf49-d61118f1802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 4\n",
    "depth = 2\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "38c93301-70f3-4c85-b19f-ab5f14bb7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get carry tables for base\n",
    "with open('../pickles/all_tables.pickle', 'rb') as f:\n",
    "    all_tables = pickle.load(f)\n",
    "tables = all_tables[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1e7bff4d-6d3d-44b2-bba2-9fa4d03e3b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tables[((0, 0, 0, 0), (0, 1, 2, 3), (0, 2, 0, 2), (0, 3, 2, 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6368a65e-0794-4b70-85eb-8bfe376d6e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataloaders\n",
    "training_dataloader, testing_dataloader = prepare_data(b, depth, table, batch_size=batch_size, num_passes=2000)\n",
    "\n",
    "# Initialize model\n",
    "model = TransformerModel(b, 2, 2)\n",
    "\n",
    "# Define criterion and optimizer\n",
    "criterion = Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7c15e22c-e45a-4d25-8e79-16466231d277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 0  loss = 3.993464\n",
      "t = 100  loss = 1.144009\n",
      "t = 200  loss = 1.325825\n",
      "t = 300  loss = 1.345300\n",
      "t = 400  loss = 1.142428\n",
      "t = 500  loss = 1.093876\n",
      "t = 600  loss = 1.174075\n",
      "t = 700  loss = 1.317793\n",
      "t = 800  loss = 1.020974\n",
      "t = 900  loss = 1.064950\n",
      "t = 1000  loss = 1.374097\n",
      "t = 1100  loss = 1.131702\n",
      "t = 1200  loss = 1.146499\n",
      "t = 1300  loss = 1.392166\n",
      "t = 1400  loss = 1.459036\n",
      "t = 1500  loss = 1.095876\n",
      "t = 1600  loss = 1.381633\n",
      "t = 1700  loss = 1.171405\n",
      "t = 1800  loss = 1.313322\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "losses = []\n",
    "t = 0\n",
    "\n",
    "# Training loop\n",
    "for batch_idx, (X, s, ids) in enumerate(training_dataloader):\n",
    "    \n",
    "    # Compute, store, and print loss\n",
    "    loss = criterion(model(X), s.float(), ids)\n",
    "    losses.append(loss.item())\n",
    "    if (t % 100 == 0):\n",
    "        print(f't = {t}  loss = {loss.item():.6f}')\n",
    "    \n",
    "    # Zero gradients, perform a backward pass, and update the weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Iterate counter\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b5478992-8901-4547-adc6-8ed6eabb94a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing set: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "test(model, testing_dataloader, depth, batch_size, print_accuracy=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
